#!/usr/bin/env -S uv run --script
# /// script
# dependencies = ["requests", "feedparser", "python-dateutil"]
# ///

import requests
import xml.etree.ElementTree as ET
import feedparser
import sys
import json
import os
from datetime import datetime, timedelta
from dateutil import parser as date_parser
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

def get_feed_frequency(feed_url):
    """Estimate posting frequency by checking entries from the last year"""
    try:
        # Parse the RSS/Atom feed
        feed = feedparser.parse(feed_url)
        
        # Check if feed parsing failed or returned error
        if hasattr(feed, 'status') and feed.status >= 400:
            return 0, "ERROR", None
        
        if not feed.entries:
            # Check if it's a parsing error vs empty feed
            if hasattr(feed, 'bozo') and feed.bozo:
                return 0, "ERROR", None
            return 0, "EMPTY", None
        
        # Get dates of entries from the last year only
        one_year_ago = datetime.now() - timedelta(days=365)
        recent_dates = []
        latest_date = None
        
        for entry in feed.entries:
            entry_date = None
            
            # Try different date fields in order of preference
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                entry_date = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                entry_date = datetime(*entry.updated_parsed[:6])
            elif hasattr(entry, 'published') and entry.published:
                try:
                    entry_date = date_parser.parse(entry.published)
                except:
                    pass
            elif hasattr(entry, 'updated') and entry.updated:
                try:
                    entry_date = date_parser.parse(entry.updated)
                except:
                    pass
            
            if entry_date:
                if not latest_date or entry_date > latest_date:
                    latest_date = entry_date
                
                # Only include entries from the last year
                if entry_date >= one_year_ago:
                    recent_dates.append(entry_date)
        
        if not recent_dates:
            return 0, "OLD", latest_date.strftime('%Y-%m-%d') if latest_date else None
        
        # Calculate posts per year based on recent entries
        posts_per_year = len(recent_dates)
        
        return posts_per_year, "OK", latest_date.strftime('%Y-%m-%d') if latest_date else None
            
    except Exception as e:
        return 0, "ERROR", None

def parse_opml(file_path):
    """Parse OPML file and extract feed information"""
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        feeds = []
        
        # Build a map of parent-child relationships
        parent_map = {c: p for p in tree.iter() for c in p}
        
        # Find all outline elements (feeds and categories)
        for outline in root.findall('.//outline'):
            # Check if it's a feed (has xmlUrl) or category
            xml_url = outline.get('xmlUrl')
            if xml_url:  # It's a feed
                feed = {
                    'title': outline.get('title', outline.get('text', 'Unknown')),
                    'url': outline.get('htmlUrl', xml_url),
                    'feed_url': xml_url,
                    'categories': []
                }
                
                # Find all parent categories by walking up the tree
                current = outline
                while current is not None:
                    parent = parent_map.get(current)
                    if parent is not None and parent.get('title') and parent.tag == 'outline':
                        category_title = parent.get('title')
                        if category_title not in feed['categories']:
                            feed['categories'].insert(0, category_title)  # Insert at beginning for proper order
                    current = parent
                
                # If no categories found, use default
                if not feed['categories']:
                    feed['categories'] = ['Uncategorized']
                
                feeds.append(feed)
        
        return feeds
        
    except Exception as e:
        print(f"Error parsing OPML: {e}")
        return []
    """Parse OPML file and extract feed information"""
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        feeds = []
        
        # Find all outline elements (feeds and categories)
        for outline in root.findall('.//outline'):
            # Check if it's a feed (has xmlUrl) or category
            xml_url = outline.get('xmlUrl')
            if xml_url:  # It's a feed
                feed = {
                    'title': outline.get('title', outline.get('text', 'Unknown')),
                    'url': outline.get('htmlUrl', xml_url),
                    'feed_url': xml_url,
                    'category': 'Uncategorized'
                }
                
                # Find parent category
                parent = outline.getparent()
                if parent is not None and parent.get('title'):
                    feed['category'] = parent.get('title')
                
                feeds.append(feed)
        
        return feeds
        
    except Exception as e:
        print(f"Error parsing OPML: {e}")
        return []

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: ./feedly-opml <path-to-opml-file> [--retry-failed] [--retry-all]")
        print("\nTo get your OPML file:")
        print("1. Go to https://feedly.com/i/opml")
        print("2. Click 'Export OPML'")
        print("3. Save the file and run: ./feedly-opml ~/Downloads/feedly.opml")
        print("\nOptions:")
        print("  --retry-failed    Only retry feeds with ERROR, EMPTY, or OLD status")
        print("  --retry-all       Retry all feeds regardless of status or date")
        sys.exit(1)
    
    opml_file = sys.argv[1]
    retry_failed = "--retry-failed" in sys.argv
    retry_all = "--retry-all" in sys.argv
    
    if retry_failed:
        # Load existing data and filter for failed feeds
        output_file = opml_file.replace('.opml', '_feeds.jsonl')
        if not os.path.exists(output_file):
            print("No existing data file found. Run without --retry-failed first.")
            sys.exit(1)
        
        failed_feeds = []
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    feed = json.loads(line)
                    if feed['status'] in ['ERROR', 'EMPTY', 'OLD']:
                        # Convert back to OPML format for processing
                        failed_feeds.append({
                            'title': feed['title'],
                            'url': feed['url'],
                            'feed_url': feed['feed_url'],
                            'categories': feed['categories']
                        })
        
        print(f"Found {len(failed_feeds)} failed feeds to retry")
        feeds = failed_feeds
    elif retry_all:
        # Load existing data and retry all feeds
        output_file = opml_file.replace('.opml', '_feeds.jsonl')
        if not os.path.exists(output_file):
            print("No existing data file found. Run without --retry-all first.")
            sys.exit(1)
        
        all_feeds = []
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    feed = json.loads(line)
                    all_feeds.append({
                        'title': feed['title'],
                        'url': feed['url'],
                        'feed_url': feed['feed_url'],
                        'categories': feed['categories']
                    })
        
        print(f"Found {len(all_feeds)} feeds to retry")
        feeds = all_feeds
    else:
        feeds = parse_opml(opml_file)
    
    # Open output files
    output_file = opml_file.replace('.opml', '_feeds.jsonl')
    log_file = opml_file.replace('.opml', '_log.txt')
    
    # Load existing data if available
    existing_data = {}
    all_feeds_data = []
    if os.path.exists(output_file):
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        feed = json.loads(line)
                        existing_data[feed['feed_url']] = feed
                        all_feeds_data.append(feed)
        except:
            pass
    
    # Process feeds in parallel
    with ThreadPoolExecutor(max_workers=10) as executor, open(log_file, 'w', encoding='utf-8') as log:
        # Write log header
        log.write(f"Feedly Feed Analysis Log - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        log.write(f"User request: Parse OPML file and analyze feed frequencies\n")
        log.write(f"Input file: {opml_file}\n")
        log.write(f"Output file: {output_file}\n")
        log.write("=" * 60 + "\n\n")
        
        print(f"Found {len(feeds)} feeds:")
        print("=" * 80)
        
        total_frequency = 0
        processed_count = 0
        file_lock = threading.Lock()
        
        def process_single_feed(feed, existing_data, retry_failed):
            """Process a single feed and return the result"""
            feed_url = feed['feed_url']
            
            # Skip if feed was processed recently (unless retrying failed feeds or all feeds)
            if not retry_failed and not retry_all and feed_url in existing_data:
                try:
                    processed_date = datetime.strptime(existing_data[feed_url]['processed_date'], '%Y-%m-%d %H:%M:%S')
                    if datetime.now() - processed_date < timedelta(days=7):
                        return None, f"Skipping {feed['title']} (processed {processed_date.strftime('%Y-%m-%d')})"
                except:
                    pass
            
            print(f"Processing: {feed['title']}")  # This will still be jumbled but shows activity
            
            # Get posting frequency
            frequency, status, last_updated = get_feed_frequency(feed['feed_url'])
            
            # Create feed entry
            feed_entry = {
                'title': feed['title'],
                'url': feed['url'],
                'feed_url': feed['feed_url'],
                'categories': feed['categories'],
                'posts_per_year': frequency,
                'status': status,
                'last_updated': last_updated,
                'processed_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            return feed_entry, f"Frequency: {frequency} posts/year ({status}) Last: {last_updated or 'Unknown'}"

        # Submit all tasks
        print(f"Starting to process {len(feeds)} feeds...")
        future_to_feed = {
            executor.submit(process_single_feed, feed, existing_data, retry_failed or retry_all): feed 
            for feed in feeds
        }
        print(f"All {len(future_to_feed)} tasks submitted, processing...")
        
        # Process completed tasks
        for future in as_completed(future_to_feed):
            feed = future_to_feed[future]
            try:
                feed_entry, message = future.result()
                
                if feed_entry is None:
                    print(message)
                    continue
                
                # Thread-safe file update
                with file_lock:
                    existing_data[feed_entry['feed_url']] = feed_entry
                    
                    # Rewrite entire file with updated data
                    with open(output_file, 'w', encoding='utf-8') as f:
                        for feed_data in existing_data.values():
                            f.write(json.dumps(feed_data, ensure_ascii=False) + '\n')
                    
                    # Log the result
                    log.write(f"Processing feed: {feed_entry['title']}\n")
                    log.write(f"  Feed URL: {feed_entry['feed_url']}\n")
                    log.write(f"  Result: {feed_entry['posts_per_year']} posts/year (Status: {feed_entry['status']}, Last: {feed_entry['last_updated']})\n\n")
                    log.flush()
                    
                    total_frequency += feed_entry['posts_per_year']
                    processed_count += 1
                    
                    # Print progress with lock to avoid jumbled output
                    print(f"[{processed_count}/{len(feeds)}] {message}", flush=True)
                    print("-" * 80)
                
            except Exception as e:
                print(f"Error processing {feed['title']}: {e}")
        
        total_frequency = 0
        processed_count = 0
        
        for i, feed in enumerate(feeds, 1):
            feed_url = feed['feed_url']
            
            # Skip if feed was processed recently (unless retrying failed feeds)
            if not retry_failed and feed_url in existing_data:
                try:
                    processed_date = datetime.strptime(existing_data[feed_url]['processed_date'], '%Y-%m-%d %H:%M:%S')
                    if datetime.now() - processed_date < timedelta(days=7):
                        print(f"{i}. Skipping {feed['title']} (processed {processed_date.strftime('%Y-%m-%d')})")
                        continue
                except:
                    pass
            
            print(f"{i}. Title: {feed['title']}")
            print(f"   URL: {feed['url']}")
            print(f"   Feed URL: {feed['feed_url']}")
            print(f"   Categories: {', '.join(feed['categories'])}")
            
            # Get posting frequency
            print("   Checking frequency...", end=" ", flush=True)
            log.write(f"Processing feed {i}/{len(feeds)}: {feed['title']}\n")
            log.write(f"  Feed URL: {feed['feed_url']}\n")
            
            frequency, status, last_updated = get_feed_frequency(feed['feed_url'])
            
            # Create feed entry
            feed_entry = {
                'title': feed['title'],
                'url': feed['url'],
                'feed_url': feed['feed_url'],
                'categories': feed['categories'],
                'posts_per_year': frequency,
                'status': status,
                'last_updated': last_updated,
                'processed_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # Update existing data
            existing_data[feed_url] = feed_entry
            
            # Rewrite entire file with updated data
            with open(output_file, 'w', encoding='utf-8') as f:
                for feed_data in existing_data.values():
                    f.write(json.dumps(feed_data, ensure_ascii=False) + '\n')
            
            # Log the result
            log.write(f"  Result: {frequency} posts/year (Status: {status}, Last: {last_updated})\n")
            log.flush()
            
            total_frequency += frequency
            processed_count += 1
            
            print(f"Frequency: {frequency} posts/year ({status}) Last: {last_updated or 'Unknown'}")
            print("-" * 80)
        
        # Final summary (file already written incrementally)
        
        # Write summary to log
        log.write("SUMMARY:\n")
        log.write(f"Total feeds processed: {processed_count}\n")
        if processed_count > 0:
            avg_freq = total_frequency / processed_count
            log.write(f"Average frequency: {avg_freq:.2f} posts/year\n")
            log.write(f"Total posts per year across all feeds: {total_frequency:.2f}\n")
    
    print(f"\nResults saved to: {output_file}")
    print(f"Log saved to: {log_file}")
    print(f"Total feeds: {processed_count}")
    if processed_count > 0:
        print(f"Average frequency: {total_frequency / processed_count:.2f} posts/year")
