#!/usr/bin/env -S uv run --script
# /// script
# dependencies = ["requests", "feedparser", "python-dateutil"]
# ///

import requests
import xml.etree.ElementTree as ET
import feedparser
import sys
import json
import os
from datetime import datetime, timedelta
from dateutil import parser as date_parser

def get_feed_frequency(feed_url):
    """Estimate posting frequency by checking entries from the last year"""
    try:
        # Parse the RSS/Atom feed
        feed = feedparser.parse(feed_url)
        
        # Check if feed parsing failed or returned error
        if hasattr(feed, 'status') and feed.status >= 400:
            return 0, "ERROR", None
        
        if not feed.entries:
            # Check if it's a parsing error vs empty feed
            if hasattr(feed, 'bozo') and feed.bozo:
                return 0, "ERROR", None
            return 0, "EMPTY", None
        
        # Get dates of entries from the last year only
        one_year_ago = datetime.now() - timedelta(days=365)
        recent_dates = []
        latest_date = None
        
        for entry in feed.entries:
            entry_date = None
            
            # Try different date fields in order of preference
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                entry_date = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                entry_date = datetime(*entry.updated_parsed[:6])
            elif hasattr(entry, 'published') and entry.published:
                try:
                    entry_date = date_parser.parse(entry.published)
                except:
                    pass
            elif hasattr(entry, 'updated') and entry.updated:
                try:
                    entry_date = date_parser.parse(entry.updated)
                except:
                    pass
            
            if entry_date:
                if not latest_date or entry_date > latest_date:
                    latest_date = entry_date
                
                # Only include entries from the last year
                if entry_date >= one_year_ago:
                    recent_dates.append(entry_date)
        
        if not recent_dates:
            return 0, "OLD", latest_date.strftime('%Y-%m-%d') if latest_date else None
        
        # Calculate posts per year based on recent entries
        posts_per_year = len(recent_dates)
        
        return posts_per_year, "OK", latest_date.strftime('%Y-%m-%d') if latest_date else None
            
    except Exception as e:
        return 0, "ERROR", None

def parse_opml(file_path):
    """Parse OPML file and extract feed information"""
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        feeds = []
        
        # Build a map of parent-child relationships
        parent_map = {c: p for p in tree.iter() for c in p}
        
        # Find all outline elements (feeds and categories)
        for outline in root.findall('.//outline'):
            # Check if it's a feed (has xmlUrl) or category
            xml_url = outline.get('xmlUrl')
            if xml_url:  # It's a feed
                feed = {
                    'title': outline.get('title', outline.get('text', 'Unknown')),
                    'url': outline.get('htmlUrl', xml_url),
                    'feed_url': xml_url,
                    'category': 'Uncategorized'
                }
                
                # Find parent category
                parent = parent_map.get(outline)
                if parent is not None and parent.get('title'):
                    feed['category'] = parent.get('title')
                
                feeds.append(feed)
        
        return feeds
        
    except Exception as e:
        print(f"Error parsing OPML: {e}")
        return []
    """Parse OPML file and extract feed information"""
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        feeds = []
        
        # Find all outline elements (feeds and categories)
        for outline in root.findall('.//outline'):
            # Check if it's a feed (has xmlUrl) or category
            xml_url = outline.get('xmlUrl')
            if xml_url:  # It's a feed
                feed = {
                    'title': outline.get('title', outline.get('text', 'Unknown')),
                    'url': outline.get('htmlUrl', xml_url),
                    'feed_url': xml_url,
                    'category': 'Uncategorized'
                }
                
                # Find parent category
                parent = outline.getparent()
                if parent is not None and parent.get('title'):
                    feed['category'] = parent.get('title')
                
                feeds.append(feed)
        
        return feeds
        
    except Exception as e:
        print(f"Error parsing OPML: {e}")
        return []

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: ./feedly-opml <path-to-opml-file>")
        print("\nTo get your OPML file:")
        print("1. Go to https://feedly.com/i/opml")
        print("2. Click 'Export OPML'")
        print("3. Save the file and run: ./feedly-opml ~/Downloads/feedly.opml")
        sys.exit(1)
    
    opml_file = sys.argv[1]
    feeds = parse_opml(opml_file)
    
    # Open output files
    output_file = opml_file.replace('.opml', '_feeds.jsonl')
    log_file = opml_file.replace('.opml', '_log.txt')
    
    # Load existing data if available
    existing_data = {}
    if os.path.exists(output_file):
        try:
            with open(output_file, 'r') as f:
                for line in f:
                    if line.strip():
                        feed = json.loads(line)
                        existing_data[feed['feed_url']] = feed
        except:
            pass
    
    with open(output_file, 'a') as f, open(log_file, 'w') as log:
        
        # Write log header
        log.write(f"Feedly Feed Analysis Log - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        log.write(f"User request: Parse OPML file and analyze feed frequencies\n")
        log.write(f"Input file: {opml_file}\n")
        log.write(f"Output file: {output_file}\n")
        log.write("=" * 60 + "\n\n")
        
        print(f"Found {len(feeds)} feeds:")
        print("=" * 80)
        
        total_frequency = 0
        processed_count = 0
        
        for i, feed in enumerate(feeds, 1):
            feed_url = feed['feed_url']
            
            # Check if we should skip (processed within last week)
            if feed_url in existing_data:
                try:
                    processed_date = datetime.strptime(existing_data[feed_url]['processed_date'], '%Y-%m-%d %H:%M:%S')
                    if datetime.now() - processed_date < timedelta(days=7):
                        print(f"{i}. Skipping {feed['title']} (processed {processed_date.strftime('%Y-%m-%d')})")
                        continue
                except:
                    pass
            
            print(f"{i}. Title: {feed['title']}")
            print(f"   URL: {feed['url']}")
            print(f"   Feed URL: {feed['feed_url']}")
            print(f"   Category: {feed['category']}")
            
            # Get posting frequency
            print("   Checking frequency...", end=" ", flush=True)
            log.write(f"Processing feed {i}/{len(feeds)}: {feed['title']}\n")
            log.write(f"  Feed URL: {feed['feed_url']}\n")
            
            frequency, status, last_updated = get_feed_frequency(feed['feed_url'])
            
            # Create feed entry
            feed_entry = {
                'title': feed['title'],
                'url': feed['url'],
                'feed_url': feed['feed_url'],
                'category': feed['category'],
                'posts_per_year': frequency,
                'status': status,
                'last_updated': last_updated,
                'processed_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # Write to JSONL immediately (one line per feed)
            f.write(json.dumps(feed_entry) + '\n')
            f.flush()
            
            # Log the result
            log.write(f"  Result: {frequency} posts/year (Status: {status}, Last: {last_updated})\n")
            log.flush()
            
            total_frequency += frequency
            processed_count += 1
            
            print(f"Frequency: {frequency} posts/year ({status}) Last: {last_updated or 'Unknown'}")
            print("-" * 80)
        
        # Write summary to log
        log.write("SUMMARY:\n")
        log.write(f"Total feeds processed: {processed_count}\n")
        if processed_count > 0:
            avg_freq = total_frequency / processed_count
            log.write(f"Average frequency: {avg_freq:.2f} posts/year\n")
            log.write(f"Total posts per year across all feeds: {total_frequency:.2f}\n")
    
    print(f"\nResults saved to: {output_file}")
    print(f"Log saved to: {log_file}")
    print(f"Total feeds: {processed_count}")
    if processed_count > 0:
        print(f"Average frequency: {total_frequency / processed_count:.2f} posts/year")
