#!/usr/bin/env -S uv run --script
# /// script
# dependencies = ["requests", "feedparser", "python-dateutil"]
# ///

import requests
import xml.etree.ElementTree as ET
import feedparser
import sys
import json
import os
from datetime import datetime, timedelta
from dateutil import parser as date_parser
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

def get_feed_frequency(feed_url):
    """Estimate posting frequency by checking entries from the last year"""
    try:
        # Parse the RSS/Atom feed
        feed = feedparser.parse(feed_url)
        
        # Check if feed parsing failed or returned error
        if hasattr(feed, 'status') and feed.status >= 400:
            return 0, "ERROR", None
        
        if not feed.entries:
            # Check if it's a parsing error vs empty feed
            if hasattr(feed, 'bozo') and feed.bozo:
                return 0, "ERROR", None
            return 0, "EMPTY", None
        
        # Get dates of entries from the last year only
        one_year_ago = datetime.now() - timedelta(days=365)
        recent_dates = []
        latest_date = None
        
        for entry in feed.entries:
            entry_date = None
            
            # Try different date fields in order of preference
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                entry_date = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                entry_date = datetime(*entry.updated_parsed[:6])
            elif hasattr(entry, 'published') and entry.published:
                try:
                    entry_date = date_parser.parse(entry.published)
                except:
                    pass
            elif hasattr(entry, 'updated') and entry.updated:
                try:
                    entry_date = date_parser.parse(entry.updated)
                except:
                    pass
            
            if entry_date:
                if not latest_date or entry_date > latest_date:
                    latest_date = entry_date
                
                # Only include entries from the last year
                if entry_date >= one_year_ago:
                    recent_dates.append(entry_date)
        
        if not recent_dates:
            return 0, "OLD", latest_date.strftime('%Y-%m-%d') if latest_date else None
        
        # Estimate posts per year based on recent posting pattern
        if len(recent_dates) < 2:
            posts_per_year = len(recent_dates)
        else:
            # Sort dates and calculate average interval between posts
            recent_dates.sort()
            intervals = []
            for i in range(1, len(recent_dates)):
                interval = (recent_dates[i] - recent_dates[i-1]).total_seconds() / 3600  # hours
                if interval > 0:
                    intervals.append(interval)
            
            if intervals:
                avg_interval_hours = sum(intervals) / len(intervals)
                posts_per_day = 24 / avg_interval_hours if avg_interval_hours > 0 else len(recent_dates)
                posts_per_year = int(posts_per_day * 365)
                
                # Debug output for --test-feed
                if len(sys.argv) > 1 and "--test-feed" in sys.argv:
                    print(f"Debug: Found {len(recent_dates)} entries in last year")
                    print(f"Debug: {len(intervals)} intervals calculated")
                    print(f"Debug: Average interval: {avg_interval_hours:.2f} hours")
                    print(f"Debug: Posts per day: {posts_per_day:.2f}")
                    print("\nPost timestamps and intervals:")
                    for i, date in enumerate(recent_dates):
                        if i == 0:
                            print(f"{i+1:3d}. {date.strftime('%Y-%m-%d %H:%M:%S')} (first post)")
                        else:
                            interval_hours = intervals[i-1]
                            print(f"{i+1:3d}. {date.strftime('%Y-%m-%d %H:%M:%S')} (+{interval_hours:.1f}h)")
            else:
                posts_per_year = len(recent_dates)
        
        return posts_per_year, "OK", latest_date.strftime('%Y-%m-%d') if latest_date else None
            
    except Exception as e:
        return 0, "ERROR", None

def parse_opml(file_path):
    """Parse OPML file and extract feed information"""
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        feeds = []
        feed_dict = {}  # Use dict to merge duplicate feeds by URL
        
        # Build a map of parent-child relationships
        parent_map = {c: p for p in tree.iter() for c in p}
        
        # Find all outline elements (feeds and categories)
        for outline in root.findall('.//outline'):
            # Check if it's a feed (has xmlUrl) or category
            xml_url = outline.get('xmlUrl')
            if xml_url:  # It's a feed
                if xml_url not in feed_dict:
                    feed_dict[xml_url] = {
                        'title': outline.get('title', outline.get('text', 'Unknown')),
                        'url': outline.get('htmlUrl', xml_url),
                        'feed_url': xml_url,
                        'categories': []
                    }
                
                # Find all parent categories by walking up the tree
                current = outline
                while current is not None:
                    parent = parent_map.get(current)
                    if parent is not None and parent.tag == 'outline' and not parent.get('xmlUrl'):
                        category_title = parent.get('title', parent.get('text'))
                        if category_title and category_title not in feed_dict[xml_url]['categories']:
                            feed_dict[xml_url]['categories'].insert(0, category_title)  # Insert at beginning for proper order
                    current = parent
                
                # If no categories found, use default (should not happen with proper OPML)
                if not feed_dict[xml_url]['categories']:
                    feed_dict[xml_url]['categories'] = ['Uncategorized']
        
        # Convert dict to list
        feeds = list(feed_dict.values())
        
        return feeds
        
    except Exception as e:
        print(f"Error parsing OPML: {e}")
        return []
    """Parse OPML file and extract feed information"""
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        feeds = []
        
        # Find all outline elements (feeds and categories)
        for outline in root.findall('.//outline'):
            # Check if it's a feed (has xmlUrl) or category
            xml_url = outline.get('xmlUrl')
            if xml_url:  # It's a feed
                feed = {
                    'title': outline.get('title', outline.get('text', 'Unknown')),
                    'url': outline.get('htmlUrl', xml_url),
                    'feed_url': xml_url,
                    'category': 'Uncategorized'
                }
                
                # Find parent category
                parent = outline.getparent()
                if parent is not None and parent.get('title'):
                    feed['category'] = parent.get('title')
                
                feeds.append(feed)
        
        return feeds
        
    except Exception as e:
        print(f"Error parsing OPML: {e}")
        return []

if __name__ == "__main__":
    if "--test-feed" in sys.argv:
        idx = sys.argv.index("--test-feed")
        if idx + 1 < len(sys.argv):
            feed_url = sys.argv[idx + 1]
            print(f"Testing feed: {feed_url}")
            frequency, status, last_updated = get_feed_frequency(feed_url)
            print(f"Result: {frequency} posts/year (Status: {status}, Last: {last_updated})")
        else:
            print("Error: --test-feed requires a URL argument")
        sys.exit(0)
    
    if len(sys.argv) < 2:
        print("Usage: ./feedly-opml <path-to-opml-file> [--retry-failed] [--retry-all]")
        print("\nTo get your OPML file:")
        print("1. Go to https://feedly.com/i/opml")
        print("2. Click 'Export OPML'")
        print("3. Save the file and run: ./feedly-opml ~/Downloads/feedly.opml")
        print("\nOptions:")
        print("  --retry-failed    Only retry feeds with ERROR, EMPTY, or OLD status")
        print("  --retry-all       Retry all feeds regardless of status or date")
        sys.exit(1)
    
    opml_file = sys.argv[1]
    retry_failed = "--retry-failed" in sys.argv
    retry_all = "--retry-all" in sys.argv
    populate_only = "--populate-only" in sys.argv
    fetch_only = "--fetch-only" in sys.argv
    
    if populate_only:
        # Phase 1: Only populate JSONL from OPML
        feeds = parse_opml(opml_file)
        output_file = opml_file.replace('.opml', '_feeds.jsonl')
        
        # Create lookup for current OPML feeds
        opml_feeds = {}
        for feed in feeds:
            opml_feeds[feed['feed_url']] = feed
        
        # Load existing data
        existing_data = {}
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            feed = json.loads(line)
                            existing_data[feed['feed_url']] = feed
            except:
                pass
        
        # Clean up: remove feeds not in OPML, update categories for remaining feeds
        cleaned_data = {}
        removed_count = 0
        updated_count = 0
        
        for feed_url, existing_feed in existing_data.items():
            if feed_url in opml_feeds:
                # Feed still exists in OPML, update categories
                opml_feed = opml_feeds[feed_url]
                if existing_feed['categories'] != opml_feed['categories']:
                    existing_feed['categories'] = opml_feed['categories']
                    updated_count += 1
                cleaned_data[feed_url] = existing_feed
            else:
                # Feed removed from OPML
                removed_count += 1
        
        # Add new feeds from OPML
        new_count = 0
        for feed in feeds:
            if feed['feed_url'] not in cleaned_data:
                # New feed, add with placeholder fetch results
                feed_entry = {
                    'title': feed['title'],
                    'url': feed['url'],
                    'feed_url': feed['feed_url'],
                    'categories': feed['categories'],
                    'posts_per_year': 0,
                    'status': 'PENDING',
                    'last_updated': None,
                    'processed_date': None
                }
                cleaned_data[feed['feed_url']] = feed_entry
                new_count += 1
        
        # Write cleaned data
        with open(output_file, 'w', encoding='utf-8') as f:
            for feed_entry in cleaned_data.values():
                f.write(json.dumps(feed_entry, ensure_ascii=False) + '\n')
        
        print(f"Populated {len(feeds)} feeds from OPML to {output_file}")
        if removed_count > 0:
            print(f"Removed {removed_count} feeds no longer in OPML")
        if updated_count > 0:
            print(f"Updated categories for {updated_count} feeds")
        if new_count > 0:
            print(f"Added {new_count} new feeds from OPML")
        sys.exit(0)
    
    if fetch_only:
        # Phase 2: Only fetch feed data for existing JSONL entries
        output_file = opml_file.replace('.opml', '_feeds.jsonl')
        if not os.path.exists(output_file):
            print("No JSONL file found. Run --populate-only first.")
            sys.exit(1)
        
        # Load feeds from JSONL
        feeds = []
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    feed = json.loads(line)
                    feeds.append(feed)
        
        print(f"Fetching data for {len(feeds)} feeds...")
        # Continue with normal processing logic below
    
    if retry_failed:
        output_file = opml_file.replace('.opml', '_feeds.jsonl')
        if not os.path.exists(output_file):
            print("No existing data file found. Run without --retry-failed first.")
            sys.exit(1)
        
        failed_feeds = []
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    feed = json.loads(line)
                    if feed['status'] in ['ERROR', 'EMPTY', 'OLD']:
                        # Convert back to OPML format for processing
                        failed_feeds.append({
                            'title': feed['title'],
                            'url': feed['url'],
                            'feed_url': feed['feed_url'],
                            'categories': feed['categories']
                        })
        
        print(f"Found {len(failed_feeds)} failed feeds to retry")
        feeds = failed_feeds
    elif retry_all:
        # Load existing data and retry all feeds
        output_file = opml_file.replace('.opml', '_feeds.jsonl')
        if not os.path.exists(output_file):
            print("No existing data file found. Run without --retry-all first.")
            sys.exit(1)
        
        all_feeds = []
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    feed = json.loads(line)
                    all_feeds.append({
                        'title': feed['title'],
                        'url': feed['url'],
                        'feed_url': feed['feed_url'],
                        'categories': feed['categories']
                    })
        
        print(f"Found {len(all_feeds)} feeds to retry")
        feeds = all_feeds
    else:
        feeds = parse_opml(opml_file)
    
    # Open output files
    output_file = opml_file.replace('.opml', '_feeds.jsonl')
    log_file = opml_file.replace('.opml', '_log.txt')
    
    # Load existing data if available
    existing_data = {}
    all_feeds_data = []
    if os.path.exists(output_file):
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        feed = json.loads(line)
                        existing_data[feed['feed_url']] = feed
                        all_feeds_data.append(feed)
        except:
            pass
    
    # Process feeds in parallel
    with ThreadPoolExecutor(max_workers=20) as executor, open(log_file, 'w', encoding='utf-8') as log:
        # Write log header
        log.write(f"Feedly Feed Analysis Log - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        log.write(f"User request: Parse OPML file and analyze feed frequencies\n")
        log.write(f"Input file: {opml_file}\n")
        log.write(f"Output file: {output_file}\n")
        log.write("=" * 60 + "\n\n")
        
        print(f"Found {len(feeds)} feeds:")
        print("=" * 80)
        
        total_frequency = 0
        processed_count = 0
        file_lock = threading.Lock()
        
        def process_single_feed(feed, existing_data, retry_failed):
            """Process a single feed and return the result"""
            feed_url = feed['feed_url']
            
            # Skip if feed was processed recently (unless retrying failed feeds or all feeds)
            if not retry_failed and not retry_all and feed_url in existing_data:
                try:
                    processed_date = datetime.strptime(existing_data[feed_url]['processed_date'], '%Y-%m-%d %H:%M:%S')
                    if datetime.now() - processed_date < timedelta(days=7):
                        return None, f"Skipping {feed['title']} (processed {processed_date.strftime('%Y-%m-%d')})"
                except:
                    pass
            
            print(f"Processing: {feed['title']}")  # This will still be jumbled but shows activity
            
            # Get posting frequency
            frequency, status, last_updated = get_feed_frequency(feed['feed_url'])
            
            # Create feed entry
            feed_entry = {
                'title': feed['title'],
                'url': feed['url'],
                'feed_url': feed['feed_url'],
                'categories': feed['categories'],
                'posts_per_year': frequency,
                'status': status,
                'last_updated': last_updated,
                'processed_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            return feed_entry, f"Frequency: {frequency} posts/year ({status}) Last: {last_updated or 'Unknown'}"

        # Submit all tasks
        print(f"Starting to process {len(feeds)} feeds...")
        future_to_feed = {
            executor.submit(process_single_feed, feed, existing_data, retry_failed or retry_all): feed 
            for feed in feeds
        }
        print(f"All {len(future_to_feed)} tasks submitted, processing...")
        
        # Process completed tasks
        for future in as_completed(future_to_feed):
            feed = future_to_feed[future]
            try:
                feed_entry, message = future.result()
                
                if feed_entry is None:
                    print(message)
                    continue
                
                # Thread-safe file update
                with file_lock:
                    existing_data[feed_entry['feed_url']] = feed_entry
                    
                    # Rewrite entire file with updated data
                    with open(output_file, 'w', encoding='utf-8') as f:
                        for feed_data in existing_data.values():
                            f.write(json.dumps(feed_data, ensure_ascii=False) + '\n')
                    
                    # Log the result
                    log.write(f"Processing feed: {feed_entry['title']}\n")
                    log.write(f"  Feed URL: {feed_entry['feed_url']}\n")
                    log.write(f"  Result: {feed_entry['posts_per_year']} posts/year (Status: {feed_entry['status']}, Last: {feed_entry['last_updated']})\n\n")
                    log.flush()
                    
                    total_frequency += feed_entry['posts_per_year']
                    processed_count += 1
                    
                    # Print progress with lock to avoid jumbled output
                    print(f"[{processed_count}/{len(feeds)}] {message}", flush=True)
                    print("-" * 80)
                
            except Exception as e:
                print(f"Error processing {feed['title']}: {e}")
        
        total_frequency = 0
        processed_count = 0
        
        for i, feed in enumerate(feeds, 1):
            feed_url = feed['feed_url']
            
            # Skip if feed was processed recently (unless retrying failed feeds)
            if not retry_failed and feed_url in existing_data:
                try:
                    processed_date = datetime.strptime(existing_data[feed_url]['processed_date'], '%Y-%m-%d %H:%M:%S')
                    if datetime.now() - processed_date < timedelta(days=7):
                        print(f"{i}. Skipping {feed['title']} (processed {processed_date.strftime('%Y-%m-%d')})")
                        continue
                except:
                    pass
            
            print(f"{i}. Title: {feed['title']}")
            print(f"   URL: {feed['url']}")
            print(f"   Feed URL: {feed['feed_url']}")
            print(f"   Categories: {', '.join(feed['categories'])}")
            
            # Get posting frequency
            print("   Checking frequency...", end=" ", flush=True)
            log.write(f"Processing feed {i}/{len(feeds)}: {feed['title']}\n")
            log.write(f"  Feed URL: {feed['feed_url']}\n")
            
            frequency, status, last_updated = get_feed_frequency(feed['feed_url'])
            
            # Create feed entry
            feed_entry = {
                'title': feed['title'],
                'url': feed['url'],
                'feed_url': feed['feed_url'],
                'categories': feed['categories'],
                'posts_per_year': frequency,
                'status': status,
                'last_updated': last_updated,
                'processed_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # Update existing data
            existing_data[feed_url] = feed_entry
            
            # Rewrite entire file with updated data
            with open(output_file, 'w', encoding='utf-8') as f:
                for feed_data in existing_data.values():
                    f.write(json.dumps(feed_data, ensure_ascii=False) + '\n')
            
            # Log the result
            log.write(f"  Result: {frequency} posts/year (Status: {status}, Last: {last_updated})\n")
            log.flush()
            
            total_frequency += frequency
            processed_count += 1
            
            print(f"Frequency: {frequency} posts/year ({status}) Last: {last_updated or 'Unknown'}")
            print("-" * 80)
        
        # Final summary (file already written incrementally)
        
        # Write summary to log
        log.write("SUMMARY:\n")
        log.write(f"Total feeds processed: {processed_count}\n")
        if processed_count > 0:
            avg_freq = total_frequency / processed_count
            log.write(f"Average frequency: {avg_freq:.2f} posts/year\n")
            log.write(f"Total posts per year across all feeds: {total_frequency:.2f}\n")
    
    print(f"\nResults saved to: {output_file}")
    print(f"Log saved to: {log_file}")
    print(f"Total feeds: {processed_count}")
    if processed_count > 0:
        print(f"Average frequency: {total_frequency / processed_count:.2f} posts/year")
